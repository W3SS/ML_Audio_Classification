{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_features(directory):\n",
    "    au_features = pd.read_csv('{}/{}/audio_features.csv'.format('../data/output/features',directory), index_col=0)\n",
    "    im_features = pd.read_csv('{}/{}/image_features.csv'.format('../data/output/features',directory), index_col=0)\n",
    "    \n",
    "    # Drop redundant columns\n",
    "    im_features = im_features.drop(['label'], axis=1)\n",
    "\n",
    "    # Merge audio and image features\n",
    "    features = pd.concat([au_features, im_features], axis=1)\n",
    "\n",
    "    # Only look at clips less than 300s long\n",
    "    features = features[features.length < 300]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeled audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = load_features('train')\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split training and test set\n",
    "from sklearn import cross_validation\n",
    "columns = ['label', 'length', 'last_ring_to_end', 'percent_silence', 'ring_count', 'white_proportion']\n",
    "train, test = cross_validation.train_test_split(features[columns], train_size=0.7, random_state=1000)\n",
    "y_train = train['label']\n",
    "X_train = train.drop('label', axis=1)\n",
    "y_test = test['label']\n",
    "X_test = test.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scale features\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "list(zip(columns[1:], lr.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=15, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "list(zip(columns[1:], rf.feature_importances_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC(kernel='rbf', probability=True) # available kernels: linear, poly, rbf, sigmoid\n",
    "svm.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Predict on the test set\n",
    "from sklearn import metrics\n",
    "for m in [lr, rf, svm]:\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    y_pred = m.predict(X_test_scaled)\n",
    "    # Area under the curve\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\n",
    "    print(metrics.roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test each model with different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "# Features to use\n",
    "columns = ['length', 'ring_count', 'last_ring_to_end', 'percent_silence', 'white_proportion']\n",
    "\n",
    "X = features[columns]\n",
    "y = features['label']\n",
    "\n",
    "for m in [LogisticRegression(),RandomForestClassifier(n_estimators=20, n_jobs=-1), SVC(kernel='rbf')]:\n",
    "    # First scale and then apply model\n",
    "    clf = make_pipeline(preprocessing.StandardScaler(), m)\n",
    "    print(m.__class__.__name__)\n",
    "    \n",
    "    # options for scoring: http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "    for scorer in ['roc_auc', 'average_precision', 'recall', 'f1']:\n",
    "        scores = cross_validation.cross_val_score(clf, X, y, cv=10, scoring=scorer, n_jobs=-1)\n",
    "        print(\"\\t{}: {:.2f} (+/- {:.2f})\".format(scorer, scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabeled Audios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unl_features = load_features('test')\n",
    "unl_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get rid of label column\n",
    "unl_features = unl_features.drop('label', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at where nan values are\n",
    "print(len(unl_features[unl_features.isnull().any(axis=1)]))\n",
    "unl_features[unl_features.isnull().any(axis=1)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just drop the remaning rows with nan values\n",
    "unl_features = unl_features.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Reset index after cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reset index for concatenating predicted labels\n",
    "unl_features = unl_features.reset_index(drop=True)\n",
    "unl_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selected models \n",
    "models = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(n_estimators=20, n_jobs=-1),\n",
    "    SVC(kernel='rbf', probability=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Features to use\n",
    "columns = ['length', 'ring_count', 'last_ring_to_end', 'percent_silence', 'white_proportion']\n",
    "\n",
    "y_train_all = features['label']\n",
    "X_train_all = features[columns]\n",
    "\n",
    "# Scale features\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_all_scaled = scaler.fit_transform(X_train_all)\n",
    "unl_features_scaled = scaler.transform(unl_features[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Balanced predicted labels \n",
    "labels_pred = []\n",
    "# Probability distribution to minimize false positives (discarded)\n",
    "proba_pred = []\n",
    "# Fit model and predict for unlabeled data\n",
    "for m in models:\n",
    "    print('Training', m.__class__.__name__)\n",
    "    m.fit(X_train_all_scaled, y_train_all)\n",
    "    labels_pred.append(m.predict(unl_features_scaled))\n",
    "    proba_pred.append(m.predict_proba(unl_features_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced labels analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for labels in labels_pred:\n",
    "    print(np.unique(labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine predictions from models to assign the final labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine labels from each predictor into a matrix (one row per predictor)\n",
    "agg_labels = np.vstack(labels_pred)\n",
    "# Sum each column of labels\n",
    "al = np.sum(agg_labels, axis=0)\n",
    "\n",
    "# If at least two predictors predict \"1\", then \"1\", else \"0\"\n",
    "al[np.where(al <= 1)] = 0\n",
    "al[np.where(al > 1)] = 1\n",
    "final_labels = al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(final_labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced labels analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# False positive threshold\n",
    "threshold = 0.9\n",
    "labels_pred_unbalanced = []\n",
    "for proba in proba_pred:\n",
    "    labels_model = []\n",
    "    for p0, p1 in proba:\n",
    "        label = 1 if p1 >= threshold else 0\n",
    "        labels_model.append(label)\n",
    "    labels_pred_unbalanced.append(labels_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for labels in labels_pred_unbalanced:\n",
    "    print(np.unique(labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine predictions from models to assign the final labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine labels from each predictor into a matrix (one row per predictor)\n",
    "agg_labels = np.vstack(labels_pred_unbalanced)\n",
    "# Sum each column of labels\n",
    "al = np.sum(agg_labels, axis=0)\n",
    "\n",
    "# If at least two predictors predict \"1\", then \"1\", else \"0\"\n",
    "al[np.where(al <= 1)] = 0\n",
    "al[np.where(al > 1)] = 1\n",
    "final_labels_unb = al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unique(final_labels_unb, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a dataFrame for the predicted labels\n",
    "pred_labels = pd.DataFrame(final_labels, columns=['pred_label'])\n",
    "pred_labels_unb = pd.DataFrame(final_labels_unb, columns=['pred_label90'])\n",
    "predicted_labels = pd.concat([pred_labels, pred_labels_unb], axis=1)\n",
    "predicted_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabeled_final = pd.concat([unl_features, predicted_labels], axis=1)\n",
    "unlabeled_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unlabeled_final.to_csv('../data/output/predicted/unlabeled_predicted.csv', index_label='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
